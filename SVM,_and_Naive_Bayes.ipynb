{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Information Gain** and its role in **Decision Trees**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is Information Gain?**\n",
        "\n",
        "**Information Gain (IG)** is a metric used to measure **how much “uncertainty” or “entropy” is reduced** when a dataset is split based on a feature.\n",
        "\n",
        "* **Entropy** measures the **impurity** or disorder in a dataset.\n",
        "* **Information Gain** tells us **how much choosing a feature improves the purity** of the split.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "[\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} , Entropy(S_v)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (S) = current dataset\n",
        "* (A) = feature being considered\n",
        "* (S_v) = subset of (S) where feature (A) has value (v)\n",
        "\n",
        "**Entropy formula (for classification with classes (c_1, c_2, ..., c_k)):**\n",
        "\n",
        "[\n",
        "Entropy(S) = -\\sum_{i=1}^k p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "* (p_i) = proportion of samples in class (i)\n",
        "* Entropy = 0 → all samples in one class (pure)\n",
        "* Entropy = 1 → evenly distributed among classes (impure)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How Information Gain is Used in Decision Trees**\n",
        "\n",
        "1. **At each node**, the decision tree evaluates all candidate features.\n",
        "2. **Compute Information Gain** for each feature.\n",
        "3. **Choose the feature with the highest Information Gain** to split the dataset.\n",
        "4. Repeat recursively for each child node until stopping criteria are met (pure nodes or max depth).\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "> “Split on the feature that gives the **largest reduction in uncertainty** about the target class.”\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Example (Intuition)**\n",
        "\n",
        "Imagine a dataset of patients with a binary target: **Has Cancer (Yes/No)**\n",
        "\n",
        "| Age | Smoking | Cancer |\n",
        "| --- | ------- | ------ |\n",
        "| 25  | Yes     | No     |\n",
        "| 45  | Yes     | Yes    |\n",
        "| 30  | No      | No     |\n",
        "| 50  | Yes     | Yes    |\n",
        "\n",
        "* **Step 1:** Compute overall entropy (uncertainty of Cancer).\n",
        "* **Step 2:** Split by “Smoking” → compute weighted entropy for “Yes” and “No” groups.\n",
        "* **Step 3:** Information Gain = Original Entropy − Weighted Entropy\n",
        "* The feature with **highest IG** is chosen for the first split.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. One-Line Summary**\n",
        "\n",
        "> Information Gain measures how much a feature reduces uncertainty in the target variable, and Decision Trees use it to select the best features for splitting nodes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VXyVmzL7ahYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear comparison between **Gini Impurity** and **Entropy**, two commonly used metrics in Decision Trees for measuring node impurity:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Gini Impurity**\n",
        "\n",
        "**Definition:**\n",
        "Gini Impurity measures how often a randomly chosen element would be incorrectly classified if it were randomly labeled according to the class distribution in the node.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "[\n",
        "Gini(S) = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "]\n",
        "\n",
        "* (p_i) = proportion of samples in class (i)\n",
        "* Range: **0 (pure) → 0.5 (max impurity for 2 classes)**\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "* Low Gini → node mostly contains one class (pure)\n",
        "* High Gini → node contains a mix of classes\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Entropy**\n",
        "\n",
        "**Definition:**\n",
        "Entropy measures the **uncertainty** or **disorder** in a node. It quantifies how mixed the classes are.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "[\n",
        "Entropy(S) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "* (p_i) = proportion of samples in class (i)\n",
        "* Range: **0 (pure) → log2(k) (maximum uncertainty)**\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "* Entropy = 0 → all samples belong to one class\n",
        "* Entropy = 1 (for 2 classes) → classes are evenly mixed\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Key Differences**\n",
        "\n",
        "| Aspect           | Gini Impurity                                                    | Entropy                                                   |\n",
        "| ---------------- | ---------------------------------------------------------------- | --------------------------------------------------------- |\n",
        "| Formula          | (1 - \\sum p_i^2)                                                 | (-\\sum p_i \\log_2(p_i))                                   |\n",
        "| Interpretation   | Probability of misclassification                                 | Measure of uncertainty (information content)              |\n",
        "| Range            | 0 → 1 (0.5 for 2-class max)                                      | 0 → log2(k) (1 for 2 classes)                             |\n",
        "| Sensitivity      | Slightly faster to compute, less sensitive to small changes in p | Slightly more sensitive to changes in class probabilities |\n",
        "| Usage in sklearn | `criterion='gini'`                                               | `criterion='entropy'`                                     |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Practical Note**\n",
        "\n",
        "* Both Gini and Entropy usually **produce similar trees**.\n",
        "* Gini is **slightly faster** to compute, so many implementations (like sklearn) default to it.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Intuition Example**\n",
        "\n",
        "For a 2-class node with class distribution ( [0.9, 0.1] ):\n",
        "\n",
        "* **Gini** = ( 1 - (0.9^2 + 0.1^2) = 0.18 ) → low impurity\n",
        "* **Entropy** = ( -0.9\\log_2 0.9 - 0.1 \\log_2 0.1 \\approx 0.47 ) → low uncertainty\n",
        "\n",
        "Both indicate a mostly pure node.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0JEz_hGa16H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "ANS-\n",
        "Here’s a clear explanation of **Pre-Pruning in Decision Trees**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**Pre-Pruning** (also called **early stopping**) is a technique where we **stop the growth of a decision tree early**, before it perfectly classifies the training data.\n",
        "\n",
        "* The goal is to **prevent overfitting**.\n",
        "* The tree is **pruned during training**, based on certain criteria, instead of growing fully and then cutting back (that’s post-pruning).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How Pre-Pruning Works**\n",
        "\n",
        "At each node during tree construction, we check **stopping conditions** before splitting further. Common criteria include:\n",
        "\n",
        "1. **Maximum depth** of the tree (`max_depth`)\n",
        "2. **Minimum number of samples** required to split a node (`min_samples_split`)\n",
        "3. **Minimum number of samples** in a leaf (`min_samples_leaf`)\n",
        "4. **Minimum Information Gain / Gini reduction** required to split\n",
        "5. **Maximum number of nodes** in the tree\n",
        "\n",
        "If any condition is met, the node becomes a **leaf**, and the tree does **not split further**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Why Pre-Pruning is Useful**\n",
        "\n",
        "* Prevents the tree from fitting **noise in the training data**.\n",
        "* Reduces **model complexity**, making the tree faster and easier to interpret.\n",
        "* Can improve **generalization** to unseen data.\n",
        "\n",
        "**Trade-off:**\n",
        "\n",
        "* Stopping too early → underfitting (tree is too simple)\n",
        "* Stopping too late → overfitting (tree is too complex)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example (sklearn)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree with pre-pruning\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=5,           # stop after depth 5\n",
        "    min_samples_split=10,  # node must have >=10 samples to split\n",
        "    criterion='gini'\n",
        ")\n",
        "dt.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Here, the tree **won’t grow beyond depth 5** or split nodes with fewer than 10 samples → pre-pruning is applied.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. One-Line Summary**\n",
        "\n",
        "> **Pre-Pruning stops the tree from growing beyond certain limits during training to reduce overfitting and improve generalization.**\n"
      ],
      "metadata": {
        "id": "SuKVRRvXbFsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. :Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier with Gini Impurity\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = dt.feature_importances_\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "6e3c6WdGbUco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Support Vector Machine (SVM)**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** tasks.\n",
        "\n",
        "* **Goal:** Find the **best boundary (hyperplane)** that separates classes in the feature space.\n",
        "* Works well for **high-dimensional data** and cases where classes are **not linearly separable** using kernel tricks.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How SVM Works**\n",
        "\n",
        "1. **Linear SVM (for two classes):**\n",
        "\n",
        "   * Finds a hyperplane that **maximizes the margin** between two classes.\n",
        "   * **Margin:** Distance between the hyperplane and the nearest points from each class (called **support vectors**).\n",
        "   * These nearest points are crucial — they **define the hyperplane**.\n",
        "\n",
        "   **Equation of hyperplane:**\n",
        "   [\n",
        "   w \\cdot x + b = 0\n",
        "   ]\n",
        "\n",
        "   * (w) = weights vector (direction of hyperplane)\n",
        "   * (b) = bias (offset)\n",
        "\n",
        "2. **Non-linear SVM:**\n",
        "\n",
        "   * Uses **kernel functions** (like RBF, polynomial, sigmoid) to transform data into higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Key Concepts**\n",
        "\n",
        "| Concept            | Explanation                                                               |\n",
        "| ------------------ | ------------------------------------------------------------------------- |\n",
        "| Support Vectors    | Points closest to the hyperplane that influence its position              |\n",
        "| Margin             | Distance between support vectors of different classes; SVM maximizes this |\n",
        "| Kernel Trick       | Maps input data to higher dimensions to handle non-linear boundaries      |\n",
        "| Regularization (C) | Controls trade-off between margin width and misclassification             |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Advantages**\n",
        "\n",
        "* Effective in **high-dimensional spaces**\n",
        "* Works well when **number of features > number of samples**\n",
        "* Robust to **overfitting** with proper regularization\n",
        "* Can handle **linear and non-linear** classification\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Intuition Example**\n",
        "\n",
        "Imagine two types of cancer patients based on gene expression:\n",
        "\n",
        "* Each patient is a point in multi-dimensional space (features = genes)\n",
        "* SVM finds a **hyperplane** that separates the patients into “Cancer Type A” vs “Cancer Type B”\n",
        "* Only the **critical boundary patients (support vectors)** determine the separation\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> SVM is a supervised learning algorithm that finds the **optimal separating hyperplane** between classes, maximizing the margin and using kernels for non-linear separations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FkgjXeiKbksx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the Kernel Trick in SVM?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of the **Kernel Trick** in SVM:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Motivation**\n",
        "\n",
        "Support Vector Machines (SVM) work by finding a **hyperplane** that separates classes.\n",
        "\n",
        "* **Problem:** Many datasets are **not linearly separable** in the original feature space.\n",
        "* Example: Imagine points forming **concentric circles** — no straight line can separate them.\n",
        "\n",
        "**Solution:** Map the data into a **higher-dimensional space** where it becomes linearly separable.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. The Kernel Trick**\n",
        "\n",
        "The **Kernel Trick** allows SVMs to **compute inner products in a high-dimensional feature space** **without explicitly mapping the data**.\n",
        "\n",
        "* Instead of transforming (x \\to \\phi(x)) (which could be very high-dimensional or infinite), we use a **kernel function** (K(x_i, x_j)) that directly computes:\n",
        "\n",
        "[\n",
        "K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle\n",
        "]\n",
        "\n",
        "* This allows SVM to operate in the **high-dimensional space efficiently** without the computational cost of actually computing (\\phi(x)).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Common Kernel Functions**\n",
        "\n",
        "| Kernel         | Formula                                         | When to use                        |\n",
        "| -------------- | ----------------------------------------------- | ---------------------------------- |\n",
        "| Linear         | (K(x_i, x_j) = x_i \\cdot x_j)                   | Linearly separable data            |\n",
        "| Polynomial     | (K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d)    | Data with polynomial relationships |\n",
        "| RBF / Gaussian | (K(x_i, x_j) = \\exp(-\\gamma |x_i - x_j|^2))     | Non-linear, complex boundaries     |\n",
        "| Sigmoid        | (K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r)) | Neural network-like boundaries     |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Intuition**\n",
        "\n",
        "* Original space: Classes overlap → not separable\n",
        "* Kernel maps data to **higher dimension** → classes become separable\n",
        "* SVM **maximizes the margin** in this higher-dimensional space\n",
        "* Computation is done efficiently using the **kernel function**, no need to explicitly compute new features\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Example Analogy**\n",
        "\n",
        "Imagine points arranged in a circle:\n",
        "\n",
        "* **Original 2D space:** Cannot draw a straight line to separate inner vs outer circle\n",
        "* **Transform to 3D (z = x² + y²):** Points now lie on a surface where a plane can separate them\n",
        "* **Kernel trick:** SVM calculates distances as if in 3D, **without actually computing the 3D coordinates**\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> The **Kernel Trick** allows SVM to efficiently handle **non-linear data** by implicitly mapping it to a higher-dimensional space using a kernel function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6PLZtoy3b0IH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        " ANS-\n",
        "\n",
        " # Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features for SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. SVM with Linear Kernel\n",
        "# -------------------------------\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. SVM with RBF Kernel\n",
        "# -------------------------------\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print(f\"SVM Accuracy with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"SVM Accuracy with RBF Kernel: {accuracy_rbf:.4f}\")\n"
      ],
      "metadata": {
        "id": "z-hYKny5cHX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of the **Naïve Bayes classifier** and why it’s called “naïve”:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is Naïve Bayes?**\n",
        "\n",
        "**Naïve Bayes** is a **probabilistic supervised learning algorithm** based on **Bayes’ Theorem**. It is mainly used for **classification tasks**.\n",
        "\n",
        "It predicts the probability that a sample belongs to a class given its features:\n",
        "\n",
        "[\n",
        "P(C \\mid X) = \\frac{P(X \\mid C) \\cdot P(C)}{P(X)}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (C) = class label\n",
        "* (X) = feature vector\n",
        "* (P(C)) = prior probability of class (C)\n",
        "* (P(X \\mid C)) = likelihood of features given class\n",
        "* (P(C \\mid X)) = posterior probability (what we want to predict)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why is it called \"Naïve\"?**\n",
        "\n",
        "It is called **“naïve”** because it assumes that **all features are independent of each other**, given the class label:\n",
        "\n",
        "[\n",
        "P(X \\mid C) = P(x_1 \\mid C) \\cdot P(x_2 \\mid C) \\cdot ... \\cdot P(x_n \\mid C)\n",
        "]\n",
        "\n",
        "* This assumption is **rarely true in real-world data**, because features often have correlations.\n",
        "* Despite this “naïve” assumption, the algorithm **works surprisingly well** in practice, especially in text classification, spam detection, and medical diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. How it Works**\n",
        "\n",
        "1. Compute **prior probabilities** (P(C)) for each class.\n",
        "2. Compute **likelihoods** (P(x_i \\mid C)) for each feature and class.\n",
        "3. For a new sample, compute **posterior probability** for each class.\n",
        "4. Assign the sample to the class with **highest posterior probability**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Advantages**\n",
        "\n",
        "* Simple, fast, and scalable to large datasets\n",
        "* Performs well on **text data** (e.g., spam detection)\n",
        "* Requires **small training data** to estimate probabilities\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Example Applications**\n",
        "\n",
        "* Email spam filtering\n",
        "* Sentiment analysis (positive/negative reviews)\n",
        "* Disease diagnosis based on symptoms\n",
        "* Document classification\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> **Naïve Bayes is a probabilistic classifier based on Bayes’ theorem, called “naïve” because it assumes feature independence, yet it performs surprisingly well in practice.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "10o87WQ4cbrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. : Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of the **differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Gaussian Naïve Bayes (GNB)**\n",
        "\n",
        "**Use Case:** Continuous features (real numbers).\n",
        "\n",
        "**Assumption:** Each feature follows a **normal (Gaussian) distribution** for each class.\n",
        "\n",
        "**Likelihood:**\n",
        "\n",
        "[\n",
        "P(x_i \\mid C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2 \\sigma_C^2}\\right)\n",
        "]\n",
        "\n",
        "* (\\mu_C) = mean of feature (i) in class (C)\n",
        "* (\\sigma_C^2) = variance of feature (i) in class (C)\n",
        "\n",
        "**Example:** Predicting whether a patient has a disease based on **continuous lab test results**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Multinomial Naïve Bayes (MNB)**\n",
        "\n",
        "**Use Case:** Discrete features (counts), often **text classification**.\n",
        "\n",
        "**Assumption:** Features are **counts or frequencies**, and follow a **multinomial distribution**.\n",
        "\n",
        "**Likelihood:**\n",
        "\n",
        "[\n",
        "P(x_i \\mid C) = \\frac{N_{i|C} + \\alpha}{N_C + \\alpha n}\n",
        "]\n",
        "\n",
        "* (N_{i|C}) = count of feature (i) in class (C)\n",
        "* (N_C) = total count of all features in class (C)\n",
        "* (\\alpha) = smoothing parameter (Laplace smoothing)\n",
        "* (n) = total number of features\n",
        "\n",
        "**Example:** Spam detection based on **word counts** in emails.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Bernoulli Naïve Bayes (BNB)**\n",
        "\n",
        "**Use Case:** Binary/boolean features (0 or 1).\n",
        "\n",
        "**Assumption:** Each feature is a **Bernoulli variable** (present or absent).\n",
        "\n",
        "**Likelihood:**\n",
        "\n",
        "[\n",
        "P(x_i \\mid C) = p_{i|C}^{x_i} (1 - p_{i|C})^{1-x_i}\n",
        "]\n",
        "\n",
        "* (x_i = 1) if feature (i) is present, else 0\n",
        "* (p_{i|C}) = probability that feature (i) is present in class (C)\n",
        "\n",
        "**Example:** Document classification where **words are either present or absent**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Key Differences Summary**\n",
        "\n",
        "| Variant     | Feature Type    | Distribution Assumption | Common Use Case                             |\n",
        "| ----------- | --------------- | ----------------------- | ------------------------------------------- |\n",
        "| Gaussian    | Continuous      | Normal/Gaussian         | Medical data, sensor readings               |\n",
        "| Multinomial | Count/frequency | Multinomial             | Text classification (word counts)           |\n",
        "| Bernoulli   | Binary (0/1)    | Bernoulli               | Text classification (word presence/absence) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuition**\n",
        "\n",
        "* **Gaussian:** “How likely is this continuous measurement for this class?”\n",
        "* **Multinomial:** “How likely is this word count pattern for this class?”\n",
        "* **Bernoulli:** “How likely is this word present or absent for this class?”\n",
        "\n"
      ],
      "metadata": {
        "id": "D01c5PUCcjag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling (optional but can help)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "rJNbF94Gc0HB"
      }
    }
  ]
}